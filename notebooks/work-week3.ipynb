{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2461c70b",
   "metadata": {},
   "source": [
    "# 과제 - 3주차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515ca4ce",
   "metadata": {},
   "source": [
    "* 데이터 확인용 기초코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16b7483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/08/28 14:01:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"rdd-dataframe\")\n",
    "        .master(\"local\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# SparkContext를 SparkSession에서 빼두기\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 하단 메시지는 Jupyter 공식이미지에서 나오는 메시지로, 무시하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json파일로 저장해 둔 스키마 불러오기\n",
    "\n",
    "import json\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Define the path to the JSON file\n",
    "input_file_path = 'github_schema.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(input_file_path, 'r') as json_file:\n",
    "    github_schema = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffed63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 스키마 확인하기\n",
    "\n",
    "def find_all_names(data, current_path=[]):\n",
    "    paths = []\n",
    "    if isinstance(data, dict):\n",
    "        if 'name' in data:\n",
    "            # Collect the path to the current 'name'\n",
    "            paths.append('.'.join(current_path + [data['name']]))\n",
    "        # Recursively search within this dictionary\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(find_all_names(value, current_path + [data.get('name', key)]))\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            paths.extend(find_all_names(item, current_path))\n",
    "    return paths\n",
    "\n",
    "for each_line in find_all_names(github_schema):\n",
    "    print(each_line.replace('fields.',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fefa7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 스키마로 파일 읽기 (빠른 확인을 위해 1개의 gh archive데이터만 사용)\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema_to_read = StructType.fromJson(github_schema)\n",
    "df = spark.read.schema(schema_to_read).json(\"../data/gh_archive/2024-07-01-14.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7931a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 스키마로 파일 읽기 (전체파일)\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema_to_read = StructType.fromJson(github_schema)\n",
    "df = spark.read.schema(schema_to_read).json(\"../data/gh_archive/2024-07-01-14.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "463bdf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         actor.login|                type|\n",
      "+--------------------+--------------------+\n",
      "|       CosmomediaCEO|           PushEvent|\n",
      "|         ayomide3211|         CreateEvent|\n",
      "|         julianwagle|           PushEvent|\n",
      "|               RTann|PullRequestReview...|\n",
      "|       HttpAnimation|           PushEvent|\n",
      "|           thunderv3|           PushEvent|\n",
      "| github-actions[bot]|           PushEvent|\n",
      "|             aspiers|PullRequestReview...|\n",
      "| github-actions[bot]|           PushEvent|\n",
      "|     acciojob-5[bot]|         CreateEvent|\n",
      "|            dfitzmau|   IssueCommentEvent|\n",
      "|      steven-bellock|   IssueCommentEvent|\n",
      "|           rssfeeder|           PushEvent|\n",
      "|     dependabot[bot]|         CreateEvent|\n",
      "|    Naveen-Palanivel|         CreateEvent|\n",
      "|          lukerQuant|   IssueCommentEvent|\n",
      "|      ZaidQourah2004|         CreateEvent|\n",
      "|youngZwiebelandth...|           PushEvent|\n",
      "|    abhishekpaturkar|         CreateEvent|\n",
      "|       TANGHANHUTNAM|           PushEvent|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "columns = ['actor.login','type']\n",
    "select_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n",
    "\n",
    "df2 = df.select(*select_exprs)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b865a8",
   "metadata": {},
   "source": [
    "## 1. spark-submit.sh 스크립트 참고해서 option 바꾸어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5435a",
   "metadata": {},
   "source": [
    "```\n",
    "<path-to-spark-submit>/spark-submit \\\n",
    "    --class <project.class.path> \\\n",
    "    --name HelloWorld \\    # job name지정\n",
    "    --master spark://spark-master:7077 \\                     # master(현재 실습기준으로는 local)\n",
    "    --driver-cores 2 \\                                       # optimization옵션에 해당\n",
    "    --driver-memory 2g \\                                     # optimization옵션에 해당\n",
    "    --num-executors 4 \\                                      # optimization옵션에 해당\n",
    "    --executor-cores 2 \\                                     # optimization옵션에 해당 (병렬처리를 얼마나 많이 할지)\n",
    "    --executor-memory 2g \\                                   # optimization옵션에 해당\n",
    "    --conf spark.driver.memoryOverhead=1g                    # \n",
    "    --conf spark.executor.memoryOverhead=1g                  # \n",
    "    --conf spark.dynamicAllocation.enabled=true \\            #\n",
    "    --conf spark.dynamicAllocation.executorIdleTimeout=2m \\  # \n",
    "    --conf spark.dynamicAllocation.minExecutors=1 \\          #\n",
    "    --conf spark.dynamicAllocation.maxExecutors=9 \\          #\n",
    "    --conf spark.dynamicAllocation.initialExecutors=1 \\      #\n",
    "    --conf spark.memory.offHeap.enabled=true \\               #\n",
    "    --conf spark.memory.offHeap.size=2G \\                    #\n",
    "    --conf spark.shuffle.service.enabled=true \\              #\n",
    "    --conf spark.driver.maxResultSize=0 \\                    #\n",
    "    --conf spark.logConf=true \\                              #\n",
    "    --jars /opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\n",
    "```\n",
    "\n",
    "\n",
    "* 미사용\n",
    "  * --class <project.class.path>   # 실행될 기본 클래스 지정\n",
    "  * --deploy-mode : client/cluster # yarn 등 미사용중       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27c75c",
   "metadata": {},
   "source": [
    "## 2. 원하는 최종 스키마를 만들기 위한 정제 코드 filter.py 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55664a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "def init_df(df):\n",
    "    df = df.select('created_at', 'id', 'payload', 'type', df.actor.login.alias('login'), df.actor.url.alias('url'), 'repo')\n",
    "    df = df.select('login', 'url', 'created_at', 'id', df.payload.repository_id.alias('repository_id'), \\\n",
    "                   df.payload.size.alias('size'), df.payload.distinct_size.alias('distinct_size'), \\\n",
    "                   df.payload.comment.alias('comment'), 'type', 'repo')\n",
    "    df = df.select('login', 'url', 'created_at', 'id', 'repository_id', 'size', 'distinct_size', 'comment', 'type', \\\n",
    "                   F.col('repo.name').alias('name'), df.repo.url.alias('repo_url'))\n",
    "    \n",
    "    n_cols = ['user_name', 'url', 'created_at', 'id', 'repository_id', 'size', 'distinct_size', 'comment', \\\n",
    "            'type', 'name', 'repo_url']\n",
    "    df = df.toDF(*n_cols)\n",
    "\n",
    "    df = df.filter(F.col(\"login\") != \"github-actions[bot]\")\n",
    "    df = df.withColumn('created_at', F.trim(F.regexp_replace(df.created_at, \"[TZ]\", \" \")))\n",
    "    df = df.withColumn('created_at', F.to_timestamp(df.created_at, 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "    # For pytorch filter\n",
    "    ## pytorch/pytorch(아이디/레포이름)으로 필터를 걸 예정\n",
    "    df = df.withColumn('userid_and_repo_name', 'repo_name')\n",
    "\n",
    "    udf_check_repo_name = F.udf(lambda name: name.split(\"/\")[-1], StringType())\n",
    "    df = df.withColumn('repo_name', udf_check_repo_name(F.col('name')))\n",
    "    df = df.drop('name')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7943339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter.py\n",
    "class PytorchTopIssuerFilter(BaseFilter):\n",
    "    def filter(self, df):\n",
    "        # Filter : repo_name = pytorch\n",
    "        base_df = df.filter(F.col('userid_and_repo_name') == 'pytorch/pytorch')\n",
    "\n",
    "        # groupby : \n",
    "        result_df = base_df.groupBy('user_name').pivot('type').count()\n",
    "        result_df = result_df.cache()\n",
    "        result_df.where((~F.col('user_name').contains('[bot]'))) \\\n",
    "                    .orderBy(F.desc('IssuesEvent')) \\\n",
    "                    .limit(10)\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036b3f3",
   "metadata": {},
   "source": [
    "## 3. 작성한 정제 코드 spark-submit 해보기\n",
    "* 제출할 내용: spark-submit.sh, filter.py (파일명은 바꾸셔도 무방합니다)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "83e523f8",
   "metadata": {},
   "source": [
    "docker exec -it metacode_de-2024-spark-master-1 spark-submit --master spark://spark-master:7077 jobs/filter.py\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8a0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91b95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2417e9de",
   "metadata": {},
   "source": [
    "## 기타(과제X) : spark-submit 테스트 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "* 구동시간을 출력하는 테스트용 파일로 옵션을 바꿔가며 spark-submit으로 테스트 해 볼 예정\n",
    "* 사용데이터 : gharchive 7월분 파일(39.3GB)\n",
    "* 편의용 커맨드 기록\n",
    "  * docker exec -it metacode_de-2024-spark-master-1 spark-submit --master spark://spark-master:7077 jobs/test_for_spark_option.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트에 사용한 코드 (test_for_spark_option.py)\n",
    "import time\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "### 시작시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "### 테스트용 코드\n",
    "\n",
    "# SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"rdd-dataframe\")\n",
    "        .master(\"local\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "# SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#json파일로 저장해 둔 스키마 불러오기\n",
    "input_file_path = './jobs/github_schema.json'\n",
    "with open(input_file_path, 'r') as json_file:\n",
    "    github_schema = json.load(json_file)\n",
    "\n",
    "# 저장한 스키마로 파일 읽기 (1달치)\n",
    "schema_to_read = StructType.fromJson(github_schema)\n",
    "df = spark.read.schema(schema_to_read).json(\"./data/gh_archive/*.json.gz\")\n",
    "\n",
    "# 데이터 확인 (actor.login컬럼의 전체데이터 distinct)\n",
    "columns = ['actor.login']\n",
    "select_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n",
    "\n",
    "df.select(*select_exprs).distinct().show(10,False)\n",
    "\n",
    "### 종료시간 기록 후 구동시간 출력\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a21131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터에서 테스트하는 코드\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "list_option = [['spark.executor.memory=2G','spark.executor.memory=3G'],\n",
    "               ['spark.executor.memory=2G','spark.executor.memory=4G'],\n",
    "               ['driver.memory=2G','driver.memory=3G'],\n",
    "               ['driver.memory=2G','driver.memory=4G'],\n",
    "               ['driver.memory=2G','driver.memory=4G'],\n",
    "               ['driver.memory=2G','driver.memory=4G'],\n",
    "               ['--executor-cores 1','--executor-cores 2'],\n",
    "               ['--executor-cores 1','--executor-cores 3'],\n",
    "    \n",
    "]\n",
    "\n",
    "dict_result = []\n",
    "for option in list_option:\n",
    "    #print(option[1])\n",
    "    ## spark-submit.sh 파일을 생성 또는 수정하는 Python 코드\n",
    "    script_content = \"\"\"#!/bin/bash\n",
    "\n",
    "    JARS=\"/opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\"\n",
    "\n",
    "    JOBNAME=\"RefinePipeline\"\n",
    "    SCRIPT=$@\n",
    "    echo ${SCRIPT}\n",
    "\n",
    "    spark-submit \\\\\n",
    "      --name ${JOBNAME} \\\\\n",
    "      --master spark://spark-master:7077 \\\\\n",
    "      --jars ${JARS} \\\\\n",
    "      --conf spark.dynamicAllocation.enabled=true \\\\\n",
    "      --conf spark.dynamicAllocation.executorIdleTimeout=2m \\\\\n",
    "      --conf spark.dynamicAllocation.minExecutors=1 \\\\\n",
    "      --conf spark.dynamicAllocation.maxExecutors=9 \\\\\n",
    "      --conf spark.dynamicAllocation.initialExecutors=1 \\\\\n",
    "      --conf spark.memory.offHeap.enabled=true \\\\\n",
    "      --conf spark.memory.offHeap.size=2G \\\\\n",
    "      --conf spark.shuffle.service.enabled=true \\\\\n",
    "      --conf spark.executor.memory=2G \\\\\n",
    "      --conf spark.driver.memory=2G \\\\\n",
    "      --conf spark.driver.maxResultSize=0 \\\\\n",
    "      --num-executors 2 \\\\\n",
    "      --executor-cores 1 \\\\\n",
    "      ${SCRIPT}\n",
    "    \"\"\"\n",
    "\n",
    "    # 파일 작성\n",
    "    with open(\"spark-submit.sh\", \"w\") as file:\n",
    "        file.write(script_content)\n",
    "\n",
    "    ## spark-submit.sh를 실행해 시간을 측정해보는 코드\n",
    "\n",
    "    # 실행할 파이썬 스크립트 경로\n",
    "    script_path = \"../jobs/test_for_spark_option.py\"\n",
    "\n",
    "    # spark-submit.sh 스크립트를 실행할 커맨드\n",
    "    command = f\"bash spark-submit.sh {script_path}\"\n",
    "\n",
    "    # 시간 측정 시작\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 커맨드 실행\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    # 시간 측정 종료\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 실행 시간 계산\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "\n",
    "    # 결과 출력 및 기록\n",
    "    dict_result[option[1]] = {'time':execution_time}\n",
    "    print(f\"{option[1]} : {execution_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1becb8fe",
   "metadata": {},
   "source": [
    "* spark-submit.sh 기본옵션 기준 > 오류남 > 퇴근하고 확인해보기\n",
    "\n",
    "24/08/29 01:21:45 INFO TaskSchedulerImpl: Cancelling stage 1\n",
    "24/08/29 01:21:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled: Job aborted due to stage failure: Task 369 in stage 1.0 failed 1 times, most recent failure: Lost task 369.0 in stage 1.0 (TID 803) (spark-master executor driver): java.io.EOFException: Unexpected end of input stream\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
    "        at java.base/java.io.InputStream.read(InputStream.java:218)\n",
    "        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
    "        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
    "        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
    "        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
    "        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
    "        at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
    "        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
    "        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
    "        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "        at java.base/java.lang.Thread.run(Thread.java:840)\n",
    "\n",
    "Driver stacktrace:\n",
    "24/08/29 01:21:45 INFO Executor: Executor is trying to kill task 370.0 in stage 1.0 (TID 804), reason: Stage cancelled: Job aborted due to stage failure: Task 369 in stage 1.0 failed 1 times, most recent failure: Lost task 369.0 in stage 1.0 (TID 803) (spark-master executor driver): java.io.EOFException: Unexpected end of input stream\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
    "        at java.base/java.io.InputStream.read(InputStream.java:218)\n",
    "        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
    "        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
    "        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
    "        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
    "        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
    "        at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
    "        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
    "        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
    "        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "        at java.base/java.lang.Thread.run(Thread.java:840)\n",
    "\n",
    "Driver stacktrace:\n",
    "24/08/29 01:21:45 INFO TaskSchedulerImpl: Stage 1 was cancelled\n",
    "24/08/29 01:21:45 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) failed in 1660.866 s due to Job aborted due to stage failure: Task 369 in stage 1.0 failed 1 times, most recent failure: Lost task 369.0 in stage 1.0 (TID 803) (spark-master executor driver): java.io.EOFException: Unexpected end of input stream\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
    "        at java.base/java.io.InputStream.read(InputStream.java:218)\n",
    "        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
    "        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
    "        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
    "        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
    "        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
    "        at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
    "        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
    "        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
    "        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "        at java.base/java.lang.Thread.run(Thread.java:840)\n",
    "\n",
    "Driver stacktrace:\n",
    "24/08/29 01:21:45 INFO Executor: Executor killed task 370.0 in stage 1.0 (TID 804), reason: Stage cancelled: Job aborted due to stage failure: Task 369 in stage 1.0 failed 1 times, most recent failure: Lost task 369.0 in stage 1.0 (TID 803) (spark-master executor driver): java.io.EOFException: Unexpected end of input stream\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
    "        at java.base/java.io.InputStream.read(InputStream.java:218)\n",
    "        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
    "        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
    "        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
    "        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
    "        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
    "        at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
    "        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
    "        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
    "        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "        at java.base/java.lang.Thread.run(Thread.java:840)\n",
    "\n",
    "Driver stacktrace:\n",
    "24/08/29 01:21:45 WARN TaskSetManager: Lost task 370.0 in stage 1.0 (TID 804) (spark-master executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 369 in stage 1.0 failed 1 times, most recent failure: Lost task 369.0 in stage 1.0 (TID 803) (spark-master executor driver): java.io.EOFException: Unexpected end of input stream\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
    "        at java.base/java.io.InputStream.read(InputStream.java:218)\n",
    "        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
    "        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
    "        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
    "        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
    "        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
    "        at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
    "        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
    "        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
    "        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "        at java.base/java.lang.Thread.run(Thread.java:840)\n",
    "\n",
    "Driver stacktrace:)\n",
    "24/08/29 01:21:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool\n",
    "Traceback (most recent call last):\n",
    "  File \"/opt/bitnami/spark/jobs/test_for_spark_option.py\", line 35, in <module>\n",
    "    df.select(*select_exprs).distinct().show(10,False)\n",
    "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 947, in show\n",
    "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 978, in _show_string\n",
    "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
    "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
    "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
    "py4j.protocol.Py4JJavaError: An error occurred while calling o42.showString.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 369 in stage 1.0 failed 1 times, most recent failure: Lost task 369.0 in stage 1.0 (TID 803) (spark-master executor driver): java.io.EOFException: Unexpected end of input stream\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
    "        at java.base/java.io.InputStream.read(InputStream.java:218)\n",
    "        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
    "        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
    "        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
    "        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
    "        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
    "        at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
    "        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
    "        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
    "        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "        at java.base/java.lang.Thread.run(Thread.java:840)\n",
    "\n",
    "Driver stacktrace:\n",
    "        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
    "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
    "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
    "        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
    "        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
    "        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
    "        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
    "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
    "        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
    "        at scala.Option.foreach(Option.scala:407)\n",
    "        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
    "        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
    "        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
    "        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
    "        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
    "Caused by: java.io.EOFException: Unexpected end of input stream\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
    "        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
    "        at java.base/java.io.InputStream.read(InputStream.java:218)\n",
    "        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
    "        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
    "        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
    "        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
    "        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
    "        at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
    "        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
    "        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
    "        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
    "        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
    "        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
    "        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
    "        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "        at java.base/java.lang.Thread.run(Thread.java:840)\n",
    "\n",
    "24/08/29 01:21:45 INFO SparkContext: Invoking stop() from shutdown hook\n",
    "24/08/29 01:21:45 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
    "24/08/29 01:21:45 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040\n",
    "24/08/29 01:21:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "24/08/29 01:21:45 INFO MemoryStore: MemoryStore cleared\n",
    "24/08/29 01:21:45 INFO BlockManager: BlockManager stopped\n",
    "24/08/29 01:21:45 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
    "24/08/29 01:21:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "24/08/29 01:21:45 INFO SparkContext: Successfully stopped SparkContext\n",
    "24/08/29 01:21:45 INFO ShutdownHookManager: Shutdown hook called\n",
    "24/08/29 01:21:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-5042288d-526e-4150-add6-8602d16687f7/pyspark-d4272a2c-4c0d-425d-a0a1-375c07ab395d\n",
    "24/08/29 01:21:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-5042288d-526e-4150-add6-8602d16687f7\n",
    "24/08/29 01:21:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-9676e47a-6a66-4361-820b-c9a9204143fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449fe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
